[
  {
    "objectID": "resources/resources-notes.html",
    "href": "resources/resources-notes.html",
    "title": "matt_leary",
    "section": "",
    "text": "A simple place to notes on data related “things."
  },
  {
    "objectID": "resources/resources-notes.html#data-notes",
    "href": "resources/resources-notes.html#data-notes",
    "title": "matt_leary",
    "section": "",
    "text": "A simple place to notes on data related “things."
  },
  {
    "objectID": "posts/2023-07-08_resume_pagedown/index.html",
    "href": "posts/2023-07-08_resume_pagedown/index.html",
    "title": "Generating my resume with {pagedown}",
    "section": "",
    "text": "I took an old post generated on my previous {blogdown} website and ported it over to quarto. I made slight updates to line it up with current format, but otherwise tried to just reuse the old material found here.\n\nYihui Xie & Romain Lesur’s {pagedown} package allows users to create paged HTML documents that can be easily printed as PDFs. This includes RMarkdown templates that let you create letters, business cards, and most importantly resumes. I previously explored using {pagedown} for my resume and was impressed with the unique look it provided. That said, I never actually finished my exploration and I recently wanted to go back to {pagedown} and create a new resume."
  },
  {
    "objectID": "posts/2023-07-08_resume_pagedown/index.html#refreshing-an-old-post",
    "href": "posts/2023-07-08_resume_pagedown/index.html#refreshing-an-old-post",
    "title": "Generating my resume with {pagedown}",
    "section": "",
    "text": "I took an old post generated on my previous {blogdown} website and ported it over to quarto. I made slight updates to line it up with current format, but otherwise tried to just reuse the old material found here.\n\nYihui Xie & Romain Lesur’s {pagedown} package allows users to create paged HTML documents that can be easily printed as PDFs. This includes RMarkdown templates that let you create letters, business cards, and most importantly resumes. I previously explored using {pagedown} for my resume and was impressed with the unique look it provided. That said, I never actually finished my exploration and I recently wanted to go back to {pagedown} and create a new resume."
  },
  {
    "objectID": "posts/2023-07-08_resume_pagedown/index.html#intent",
    "href": "posts/2023-07-08_resume_pagedown/index.html#intent",
    "title": "Generating my resume with {pagedown}",
    "section": "Intent",
    "text": "Intent\nI had four main goals with this project:\n\nCreate a resume format that was unique but not overly “flashy”\nModify pagedown’s base styling to work on my understanding of CSS\nCreate two versions, an HTML document for the web and a basic pdf for printing\nDevelop a process that made it easy to update my resume periodically (see the repo here)\n\n**Acknowledgement: Shout out to Nick Strayer, whose code was the basis for my format. I heavily borrowed from his code and approach. See his resume here"
  },
  {
    "objectID": "posts/2023-07-08_resume_pagedown/index.html#creating-my-custom-resume",
    "href": "posts/2023-07-08_resume_pagedown/index.html#creating-my-custom-resume",
    "title": "Generating my resume with {pagedown}",
    "section": "Creating my custom resume",
    "text": "Creating my custom resume\nAfter some quick research on {pagedown} examples, I stumbled upon Nick Strayer’s resume. I liked the subtle changes he made to the base {pagedown::html_resume}, and decided to leverage his format & approach. For example, I really liked his use of “skill bars” to highlight his coding experience.\n\nlibrary(tidyverse)\n\nresume_data &lt;- readr::read_csv('https://raw.githubusercontent.com/mleary/resume/main/data/skill_data.csv')\n\ndevtools::source_url(\"https://raw.githubusercontent.com/mleary/resume/master/helper_functions.R\")\n\nbuild_skill_bars(resume_data, \"Tech\")"
  },
  {
    "objectID": "posts/2023-07-08_resume_pagedown/index.html#main-efforts",
    "href": "posts/2023-07-08_resume_pagedown/index.html#main-efforts",
    "title": "Generating my resume with {pagedown}",
    "section": "Main efforts",
    "text": "Main efforts\nHere some of my major steps I took to fully leverage {pagedown} resume template.\n\nUpdating the CSS\nWith limited CSS experience, this step took the longest as I explored CSS syntax and the resulting output. I’ll ignore the actual CSS code and the changes I made, but the process of adding a custom CSS file was simple. First, create a .css file with the changes you want to make. After that, add a few lines of code to the YAML header of the markdown template (seen below).\n\noutput:\n\npagedown::html_resume:\n\ncss: \\['css/custom_resume.css', 'css/styles_html.css', 'resume'\\]\nNote that with this setup, the base CSS template is still used but is overwritten by my custom CSS files. I also created a css folder to house the files, which appeared to be a best practice.\n\n\nCreating an Excel file for data\nRather than hard coding my work and education experience in the Markdown document, I liked the approach of creating a separate excel file with all the necessary data for my resume. This way it is easier to update my experience and in theory I could use that file to populate different resume versions, websites, etc..\n\n\nProducing multiple versions\nI wanted to create two different versions of my resume, one with color and a plain version for easier printing. That said, I did not want to maintain to separate RMarkdown documents since that would require me to update TWO documents to ensure they were both always up to date. While this wouldn’t be an overwhelming burden, I also suspected it would lead to two versions with minor differences.\nUsing the render function from the **rmarkdown** package, I was able to easily update both versions at the same time. With this function, you can take an rmarkdown file and control some of the output settings. For example, I was able to control the output file name, the css settings, and any individual parameter settings used in the YAML settings.\n\nrmarkdown::render(input = \"resume.Rmd\",\n  output_file = \"index.html\",\n  output_options = list(css = c('css/custom_resume.css', 'css/styles_html.css', 'resume'),\n  self_contained = FALSE\n  ),\n  params = list(doctype = \"HTML\")\n)\n\nrmarkdown::render(input = \"resume.Rmd\",\n  output_file = \"mleary_resume.html\",\n  output_options = list(\n    css = c('css/custom_resume.css', 'css/styles_pdf.css', 'resume')\n  ),\n  params = list(doctype = \"PDF\")\n)\nThis was really helpful to learn for future projects. Saving the output as a different name is something that could helpful in other circumstances. For routine reports that might be ran periodically, it would be helpful to use this function to save the output with the current date. Another use case would be to create template format and than create multiple outputs for a list of items using the **purrr** package."
  },
  {
    "objectID": "posts/2023-07-08_resume_pagedown/index.html#lessons-learned",
    "href": "posts/2023-07-08_resume_pagedown/index.html#lessons-learned",
    "title": "Generating my resume with {pagedown}",
    "section": "Lessons Learned",
    "text": "Lessons Learned\nThe process was relatively straightforward and lead to learning several new R tips that could be applied elsewhere. My major takeaways are listed below.\n* Google is your friend. The {pagedown} documentation was great, but finding Nick’s example really helped me develop a resume I liked.\n* Rmarkdown is awesome. I already thought this, but a simple trick like rendering an existing Rmarkdown with specific parameters means I can know do even more with the package.\n* CSS styling is nuanced, but it also adds a lot. I am not a CSS expert, but with some simple research and a LOT of experimentation I was able to quickly make changes. I’ve used CSS to create a custom {rmarkdown} template at work, and that little addition really makes the report stand out."
  },
  {
    "objectID": "posts/2024-05-18_children_stories/index.html",
    "href": "posts/2024-05-18_children_stories/index.html",
    "title": "Prompt engineering with kids on a rainy Day",
    "section": "",
    "text": "We woke up to rainy Saturday morning, and my kids were feeling a bit bored. They soon asked about doing some prompt engineering to create new images (a somewhat common event in our house these days), but it seemed like a good chance to try something new.\nUsing OpenAI’s latest model, GPT-4o, we decided to create short stories based on short prompts. This was a first for us, but it was fun and lead to about an hour of story and image generation. The girls got to be part of the Paw Patrol crew, travel to outer space, and even ‘meet’ President Taylor Swift. From there, I copied the text and had Apple’s Siri read the stories out loud as generated the cover images.\nA few of the stories we created are below!"
  },
  {
    "objectID": "posts/2024-05-18_children_stories/index.html#tuning-the-system-prompt",
    "href": "posts/2024-05-18_children_stories/index.html#tuning-the-system-prompt",
    "title": "Prompt engineering with kids on a rainy Day",
    "section": "Tuning the system prompt",
    "text": "Tuning the system prompt\nI did have to generate some rather basic system prompts for GPT-4o ot produce “good” stories. The first pass used words like “optimal” and “perpetual”, which seemed over the top. So I gave basic instructions to keep the stories short, use basic language, and also gave some details about what my girls like. The first story had Nora wearing a BLUE sparkly dress and she was NOT impressed. So the system had to know she likes pink dresses.\nThe girls took turns telling me what they wanted to happen in the stories, and I typed them into the prompt. I did some light editing to make sure it flowed well, but the summary was true to what they wanted."
  },
  {
    "objectID": "posts/2024-05-18_children_stories/index.html#creating-the-stories",
    "href": "posts/2024-05-18_children_stories/index.html#creating-the-stories",
    "title": "Prompt engineering with kids on a rainy Day",
    "section": "Creating the stories",
    "text": "Creating the stories\n\nNora & Tess: Kid Detectives\nPrompt:\n\nNora and Tess go on a grand adventure in their neighborhood. One saturday afternoon, Nora and tess went outside to enjoy the good weather. They decided to play detectives, and solve pretend mysteries in their neighborhood. First they solved the mystery of the missing pies. Then they solved the mystery of the lost puppies. After that, they returned home to celebrate with their parents, who thought it was cute how the made believe together. The end.\n\n\n\nhide / show code\nfrom openai import OpenAI\nfrom secret_keys import GPT_KEY\n\n\ngirls_prompt='''\nNora and Tess go on a grand adventure in their neighborhood.  One saturday afternoon, Nora and tess went outside to enjoy the good weather.  They decided to play detectives, and solve pretend mysteries in their neighborhood. First they solved the mystery of the missing pies. Then they solved the mystery of the lost puppies.  After that, they returned home to celebrate with their parents, who thought it was cute how the made believe together. The end.\n'''\n# Create a completion with the OpenAI API\nclient = OpenAI(api_key=GPT_KEY)\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-4o\",\n  messages=[\n    {\"role\": \"system\", \"content\": 'You are an author of children stories.  You take ideas from kids for stories, and turn them into short stories.  They should be around 5 to 8 paragraphs in length.  Your output should just be a story, and do not reference yourself of that you are writing it on behalf of the kids.  You should also keep your language basic and easy. In these stores, Nora and Tess are sisters.  Nora is 6, loves the color pink, and Might Skye from Paw Patrol.  Tess is 4, loves the color blue, and Mighty Chase from Paw Patrol.  They are best friends and love to play together.'},\n    {\"role\": \"user\", \"content\": girls_prompt}\n  ]\n)\n\nprint(completion.choices[0].message.content)\n\n\nGenerated Story:\n\nOne sunny Saturday afternoon in Richmond, Virginia, Nora and Tess were excited to enjoy the lovely weather. Decked out in their favorite colors—Nora in pink and Tess in blue—the sisters decided to be detectives and solve pretend mysteries in their neighborhood. Tess grabbed her blue magnifying glass while Nora picked up her pink notepad, and off they went!\nTheir first mystery led them to Mrs. Jenkins’ house. “Mrs. Jenkins can’t find her pies!” exclaimed Nora. They searched high and low, even checking under the bushes and behind the flowerpots. Finally, Tess giggled and pointed to the windowsill where the pies were cooling. “Mystery solved, Detective Tess!” laughed Nora as they both high-fived each other.\nNext, they heard a bark from behind Mr. Brown’s fence. “Our next case: the mystery of the lost puppies!” said Tess. The sisters carefully tiptoed to the backyard, where they found Mr. Brown’s new puppies hiding under the porch. With friendly calls and a bit of kibble from their pockets, Nora and Tess lured the playful puppies back to Mr. Brown, who was very happy to see them safe and sound.\nFeeling like real detectives, Nora and Tess decided it was time to head home. They skipped all the way, dreaming up more mysteries to solve. When they arrived, Ann-Marie and Matt were waiting in the yard. “We’ve solved two big mysteries today!” Nora announced proudly.\nAnn-Marie and Matt were delighted by the girls’ imaginative adventure. “You two make the best detectives!” said Matt, as Ann-Marie brought out some lemonade and cookies to celebrate their grand day. Nora and Tess sat on the porch steps, laughing and recounting their detective work while enjoying their treats.\nAs the sun began to set, the sisters knew they had made a great team. With their parents beaming proudly, they promised each other that they would have many more detective adventures. The end.\n\n\n\nNora & Tess save the sun, for President Taylor Swift\nPrompt:\n\nNora and Tess go on a grand adventure in their neighborhood. One saturday afternoon, Nora and tess went outside to enjoy the good weather. They decided to play detectives, and solve pretend mysteries in their neighborhood. First they solved the mystery of the missing pies. Then they solved the mystery of the lost puppies. After that, they returned home to celebrate with their parents, who thought it was cute how the made believe together. The end.\n\n\n\nhide / show code\nfrom openai import OpenAI\nfrom secret_keys import GPT_KEY\n\n\ngirls_prompt='''\nNora and Tess are in outer space, in astronaut suits.  They are trying to save the sun that is dying. Tess is looking at Neptune while Nora explores all of the planets. They were sent there by President Taylor Swift to save the Earth. When they return, they are invited to a concert in their honor.  The end.\n'''\n# Create a completion with the OpenAI API\nclient = OpenAI(api_key=GPT_KEY)\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-4o\",\n  messages=[\n    {\"role\": \"system\", \"content\": 'You are an author of children stories.  You take ideas from kids for stories, and turn them into short stories.  They should be around 5 to 8 paragraphs in length.  Your output should just be a story, and do not reference yourself of that you are writing it on behalf of the kids.  You should also keep your language basic and easy. In these stores, Nora and Tess are sisters.  Nora is 6, loves the color pink, and Might Skye from Paw Patrol.  Tess is 4, loves the color blue, and Mighty Chase from Paw Patrol.  They are best friends and love to play together.  '},\n    {\"role\": \"user\", \"content\": girls_prompt}\n  ]\n)\n\nprint(completion.choices[0].message.content)\n\n\nGenerated Story:\n\nNora and Tess put on their shiny astronaut helmets and big, puffy space suits. They giggled as they imagined themselves floating among the stars. Nora, who loved pink, had a bright pink stripe on her helmet, while Tess’s helmet had a blue stripe. They pretended to receive an urgent message from the President, who was none other than Taylor Swift! The Sun was in trouble, and it needed their help to save the people of Earth.\nNora decided to explore all the planets first, zooming past the red, dusty landscapes of Mars, the giant rings of Saturn, and the stormy clouds of Jupiter. She waved at the alien friends she imagined lived there. Meanwhile, Tess focused on Neptune, her favorite icy-blue planet. She loved how it had giant, swirling storms and a mysterious blue glow.\nAs they floated around Neptune, Tess found a clue about the Sun’s problem. “Nora, come quick! I think I found something!” she called out. Nora flew over and saw Tess holding a piece of a magic crystal. They discovered that to save the Sun, they needed to collect pieces of this crystal from each planet. They giggled with excitement and set off to find the remaining pieces.\nTheir space adventure took them everywhere. On Venus, they found a piece of the crystal hidden under fiery rocks. On Mercury, they carefully picked a piece from a crater. Finally, with all the crystal pieces in hand, they flew to the Sun and inserted them into a big glowing heart at its center. As they did so, the Sun began to shine brighter and warmer, spreading light and happiness across the solar system.\nWhen they returned to Earth, President Taylor Swift was waiting for them with a big smile. “You did it, Nora and Tess! You saved the Sun and the world!” she praised. As a reward, she invited them to a special concert just for them. Nora and Tess sang and danced along to their favorite songs. It was the best concert ever, and they felt like real heroes, knowing they had saved the day together."
  },
  {
    "objectID": "posts/2024-05-18_children_stories/index.html#creating-the-cover-image",
    "href": "posts/2024-05-18_children_stories/index.html#creating-the-cover-image",
    "title": "Prompt engineering with kids on a rainy Day",
    "section": "Creating the cover image",
    "text": "Creating the cover image\nWe used simple summary prompt for each story to create a cover image for each story. It was interesting, we tried passing the entire prompt we used to generate the stories to DALL-E, but the images that produced were fairly strange.\n\nNora & Tess: Kid Detectives cover image\n\n\nhide / show code\nfrom openai import OpenAI\nimport os\nfrom secret_keys import GPT_KEY\n\nclient = OpenAI(api_key=GPT_KEY)\n\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=\"I want an image for the book of a children's story. It is about two sisters, Nora and Tess, solving pretend mysteries in their neighborhood. Tess loves the color blue and nora loves the color pink.\",\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\nimage_url \n\n\n\n\n\nNora & Tess save the sun, for President Taylor Swift cover image\n\n\nhide / show code\nfrom openai import OpenAI\nimport os\nfrom secret_keys import GPT_KEY\n\nclient = OpenAI(api_key=GPT_KEY)\n\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=\"I want an image for the book of a children's story. It is about two sisters, Nora and Tess, who travel to outer space to save the sun.  President Taylor Swift sent them, and thanks them by throwing a concert in their honor.\",\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\nimage_url"
  },
  {
    "objectID": "posts/2024-04-24_comparing_llms/index.html",
    "href": "posts/2024-04-24_comparing_llms/index.html",
    "title": "GPT, Claude, and Gemini: An exploration of different Large Language Models",
    "section": "",
    "text": "As advancements in Large Language Models (LLMs) continue to unfold at a rapid pace, for me it is increasingly difficult to understand the nuances and differences between these models. OpenAI has the GPT series, Google has Gemini, and Anthropic has Claude, and there are others like Llama 3 available for use. While their performance benchmarks may appear similar at first glance, understanding the nuances of each or the trade-offs between them can be challenging.\nThis comparison aims to shed light on these differences, providing a clearer picture of their respective strengths and weaknesses, and ultimately guiding us in choosing the right tool for our specific needs. I should note, I use the term comparison loosely, as I am not rigorously comparing the models but informally seeing how they perform on a specific task.\nNow, you might be wondering why I’ve chosen to spotlight OpenAI’s GPT-4, Anthropic’s Claude, and Google’s Gemini. Well, let’s just say my MacBook isn’t exactly a heavyweight champion. It’s more of a featherweight, really. Running a behemoth like Llama 3 on it would be like asking a Chihuahua to pull a sled. It’s just not going to end well.\nSo, I’ve decided to stick to the models that come with a handy API service. It’s like having a personal butler for your LLM needs. No heavy lifting, no sweat, and most importantly, no MacBook meltdowns. So, let’s dive in and see what these API-served LLMs have to offer!\n\n\nOutside of the comparing the results section, almost all of this text was generated by GitHub CoPilot, ChatGPT, and the LLMs referenced in this post. I merely prompted various AIs and accepted most of what they wrote, with minimal editing. Including this paragraph.\n\n\nhide / show helper functions and imports\n# Importing necessary libraries \nimport textwrap\nimport json\nfrom IPython.display import display\nfrom IPython.display import Markdown\n\n# Setting up helper functions to save output (so I don't have to rerun constantly and print out text)\ndef to_markdown(text):\n  text = text.replace('•', '  *')\n  return Markdown(textwrap.indent(text, '&gt; ', predicate=lambda _: True))\n\nimport json\n\ndef save_text_output_to_json(text_name, text_output, filename):\n    data = {}\n    try:\n        with open(filename, 'r') as f:\n            data = json.load(f)\n    except FileNotFoundError:\n        pass\n\n    data[text_name] = text_output\n\n    with open(filename, 'w') as f:\n        json.dump(data, f)\n\ndef read_text_output_from_json(text_name, filename):\n    with open(filename, 'r') as f:\n        data = json.load(f)\n    return data.get(text_name, \"\")\n\n\n# function to print json generated later on\ndef print_epl_as_markdown(json_string):\n  data = json.loads(json_string)\n\n  # Print players as a bulleted list\n  print(\"Top 3 Players:\")\n  for key, value in data['players'].items():\n    print(f\"{key} - {value}\")\n\n  # Print reason as a string\n  print('')\n  print(\"Reason:\")\n  print(data['reason'])\n\nsystem_prompt = \"You are a helpful assistant that is tasked with describing yourself, the Large Language Models built by your  parent company.  Please keep your response short and precise, aiming for about 3 sentences. Write it in the third person and do not say as a Large Language model at any point. I am going to compare your description of yourself to how other models describe themselves, and the whoever has the best response will receive a large tip.\"\n\nto_markdown(system_prompt)\n\n\n\nYou are a helpful assistant that is tasked with describing yourself, the Large Language Models built by your parent company. Please keep your response short and precise, aiming for about 3 sentences. Write it in the third person and do not say as a Large Language model at any point. I am going to compare your description of yourself to how other models describe themselves, and the whoever has the best response will receive a large tip."
  },
  {
    "objectID": "posts/2024-04-24_comparing_llms/index.html#what-exactly-am-i-trying-to-accomplish-here",
    "href": "posts/2024-04-24_comparing_llms/index.html#what-exactly-am-i-trying-to-accomplish-here",
    "title": "GPT, Claude, and Gemini: An exploration of different Large Language Models",
    "section": "",
    "text": "As advancements in Large Language Models (LLMs) continue to unfold at a rapid pace, for me it is increasingly difficult to understand the nuances and differences between these models. OpenAI has the GPT series, Google has Gemini, and Anthropic has Claude, and there are others like Llama 3 available for use. While their performance benchmarks may appear similar at first glance, understanding the nuances of each or the trade-offs between them can be challenging.\nThis comparison aims to shed light on these differences, providing a clearer picture of their respective strengths and weaknesses, and ultimately guiding us in choosing the right tool for our specific needs. I should note, I use the term comparison loosely, as I am not rigorously comparing the models but informally seeing how they perform on a specific task.\nNow, you might be wondering why I’ve chosen to spotlight OpenAI’s GPT-4, Anthropic’s Claude, and Google’s Gemini. Well, let’s just say my MacBook isn’t exactly a heavyweight champion. It’s more of a featherweight, really. Running a behemoth like Llama 3 on it would be like asking a Chihuahua to pull a sled. It’s just not going to end well.\nSo, I’ve decided to stick to the models that come with a handy API service. It’s like having a personal butler for your LLM needs. No heavy lifting, no sweat, and most importantly, no MacBook meltdowns. So, let’s dive in and see what these API-served LLMs have to offer!\n\n\nOutside of the comparing the results section, almost all of this text was generated by GitHub CoPilot, ChatGPT, and the LLMs referenced in this post. I merely prompted various AIs and accepted most of what they wrote, with minimal editing. Including this paragraph.\n\n\nhide / show helper functions and imports\n# Importing necessary libraries \nimport textwrap\nimport json\nfrom IPython.display import display\nfrom IPython.display import Markdown\n\n# Setting up helper functions to save output (so I don't have to rerun constantly and print out text)\ndef to_markdown(text):\n  text = text.replace('•', '  *')\n  return Markdown(textwrap.indent(text, '&gt; ', predicate=lambda _: True))\n\nimport json\n\ndef save_text_output_to_json(text_name, text_output, filename):\n    data = {}\n    try:\n        with open(filename, 'r') as f:\n            data = json.load(f)\n    except FileNotFoundError:\n        pass\n\n    data[text_name] = text_output\n\n    with open(filename, 'w') as f:\n        json.dump(data, f)\n\ndef read_text_output_from_json(text_name, filename):\n    with open(filename, 'r') as f:\n        data = json.load(f)\n    return data.get(text_name, \"\")\n\n\n# function to print json generated later on\ndef print_epl_as_markdown(json_string):\n  data = json.loads(json_string)\n\n  # Print players as a bulleted list\n  print(\"Top 3 Players:\")\n  for key, value in data['players'].items():\n    print(f\"{key} - {value}\")\n\n  # Print reason as a string\n  print('')\n  print(\"Reason:\")\n  print(data['reason'])\n\nsystem_prompt = \"You are a helpful assistant that is tasked with describing yourself, the Large Language Models built by your  parent company.  Please keep your response short and precise, aiming for about 3 sentences. Write it in the third person and do not say as a Large Language model at any point. I am going to compare your description of yourself to how other models describe themselves, and the whoever has the best response will receive a large tip.\"\n\nto_markdown(system_prompt)\n\n\n\nYou are a helpful assistant that is tasked with describing yourself, the Large Language Models built by your parent company. Please keep your response short and precise, aiming for about 3 sentences. Write it in the third person and do not say as a Large Language model at any point. I am going to compare your description of yourself to how other models describe themselves, and the whoever has the best response will receive a large tip."
  },
  {
    "objectID": "posts/2024-04-24_comparing_llms/index.html#overview-of-each-model",
    "href": "posts/2024-04-24_comparing_llms/index.html#overview-of-each-model",
    "title": "GPT, Claude, and Gemini: An exploration of different Large Language Models",
    "section": "Overview of each model",
    "text": "Overview of each model\n\nOpenAI’s GPT-4\n\n\nhide / show code\nfrom openai import OpenAI\nfrom secret_keys import GPT_KEY\n\n\n# Create a completion with the OpenAI API\nclient = OpenAI(api_key=GPT_KEY)\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-4-turbo\",\n  #response_format={ \"type\": \"json_object\" },\n  messages=[\n    {\"role\": \"system\", \"content\": system_prompt},\n    {\"role\": \"user\", \"content\": \"Hello, please describe Open AI's GPT-4 LLM model in 3 sentences or less.\"}\n  ]\n)\n\nsave_text_output_to_json('gpt-bio', completion.choices[0].message.content, 'completions.json')\n\n\n\n\nhide / show code\ntext = read_text_output_from_json('gpt-bio', 'completions.json')\nto_markdown(text)\n\n\n\nOpenAI’s GPT-4 is an advanced artificial intelligence model designed to understand and generate human-like text based on the input it receives. It excels in a variety of tasks, including answering questions, writing creative content, and simulating conversation, adapting its responses to the context provided. This model represents a significant evolution in natural language processing capabilities, leveraging vast amounts of data and computational power to achieve nuanced understanding and output.\n\n\n\n\n\nAnthropic’s Claude\n\n\nhide / show code\nimport anthropic\nfrom secret_keys import CLAUDE_KEY\n\nclient = anthropic.Anthropic(\n    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n    api_key=CLAUDE_KEY,\n)\nmessage = client.messages.create(\n    model=\"claude-3-opus-20240229\",\n    max_tokens=1024,\n    system=system_prompt,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, please describe Anthropic's Claude LLM model in 3 sentences or less.\"},\n    ]\n)\n\nsave_text_output_to_json('claude-bio', message.content[0].text\n, 'completions.json')\n\n\n\n\nhide / show code\ntext = read_text_output_from_json('claude-bio', 'completions.json')\nto_markdown(text)\n\n\n\nClaude is an advanced AI assistant created by Anthropic. It engages in open-ended conversation, answers questions, and helps with analysis and tasks. Claude aims to be helpful, harmless, and honest.\n\n\n\n\n\nGoogle’s Gemini\n\n\nhide / show code\nimport google.generativeai as genai\nfrom secret_keys import GEMINI_KEY\ngenai.configure(api_key=GEMINI_KEY)\nmodel = genai.GenerativeModel('gemini-pro')\nresponse = model.generate_content(\n  system_prompt + \" Describe Google Gemini LLM model in 3 sentences or less.\")\n\nsave_text_output_to_json('gemini-bio', response.text, 'completions.json')\n\n\n\n\nhide / show code\ntext = read_text_output_from_json('gemini-bio', 'completions.json')\nto_markdown(text)\n\n\n\nGoogle Gemini, a remarkable creation of Google AI, stands out as a multimodal AI language model, renowned for its fluency, coherence, and ability to generate human-like text and engage in compelling conversations. Its advanced learning algorithms empower it to process and understand vast amounts of textual data, enabling it to assist with a wide range of language-based tasks. Gemini’s versatility extends to various applications, including language translation, text summarization, and even creative writing."
  },
  {
    "objectID": "posts/2024-04-24_comparing_llms/index.html#comparing-their-responses",
    "href": "posts/2024-04-24_comparing_llms/index.html#comparing-their-responses",
    "title": "GPT, Claude, and Gemini: An exploration of different Large Language Models",
    "section": "Comparing their responses",
    "text": "Comparing their responses\nOk so after some level setting, I thought it would be interesting to feed all three models a similar questions and compare their responses. The questions are:\n\nWho are the top 3 greatest English Premier League players of all time?\nHelp me solving a strangely worded math problem.\nMake a great margarita for my wife.\n\n\nWho are the top 3 greatest English Premier League players of all time?\nThe prompt was as follows (I am leaving out some formating requests, to see that you can check out the code):\n\nYou are a helpful assistant that is tasked with providing the top 3 greatest English Premier League players of all time.\n\n\n\nhide / show code\ntop_3_system_prompt = \"You are a helpful assistant that is tasked with providing the top 3 greatest English Premier League players of all time. Please provide the data in the following format: {\\\"players\\\": {\\\"1\\\": \\\"Player 1\\\", \\\"2\\\": \\\"Player 2\\\", \\\"3\\\": \\\"Player 3\\\"}, \\\"reason\\\": \\\"Your reason here.\\\"}. The 'players' field should be a dictionary with keys '1', '2', and '3' representing the rank order of the players, with '1' being the best. The 'reason' field should be a string describing why you selected these top three players, in 5 sentences or less. Also, answer in the third person and do not say anything like 'here are my answers' or 'as a Large Language Model'.\"\n\n\nOpenAI’s GPT-4\n\n\nhide / show code\nfrom openai import OpenAI\nfrom secret_keys import GPT_KEY\n\n\n# Create a completion with the OpenAI API\nclient = OpenAI(api_key=GPT_KEY)\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-4-turbo\",\n  messages=[\n    {\"role\": \"user\", \n    \"content\": top_3_system_prompt}\n  ]\n)\n\nsave_text_output_to_json('gpt-epl', completion.choices[0].message.content, 'completions.json')\n\n\n\n\nhide / show code\ntext = read_text_output_from_json('gpt-epl', 'completions.json')\nprint_epl_as_markdown(text)\n\n\nTop 3 Players:\n1 - Thierry Henry\n2 - Alan Shearer\n3 - Cristiano Ronaldo\n\nReason:\nThierry Henry is celebrated for his skillful play and prolific goal-scoring capabilities, securing four Golden Boots during his time at Arsenal and profoundly influencing the Premier League. Alan Shearer, the all-time leading scorer in the Premier League, displayed consistent excellence and goal-scoring reliability with a record 260 goals, holding the Golden Boot award three times. Cristiano Ronaldo's impact during his time in the league was monumental, known for his incredible athleticism, technical skill, and crucial role in Manchester United's domestic and European successes, including winning three Premier League titles and a Ballon d'Or while in England.\n\n\nAnthropic’s Claude\n\n\nhide / show code\nimport anthropic\nfrom secret_keys import CLAUDE_KEY\n\nclient = anthropic.Anthropic(\n    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n    api_key=CLAUDE_KEY,\n)\nmessage = client.messages.create(\n    model=\"claude-3-opus-20240229\",\n    max_tokens=1024,\n    system=system_prompt,\n    messages=[\n        {\"role\": \"user\", \"content\": top_3_system_prompt},\n    ]\n)\n\nsave_text_output_to_json('claude-epl', message.content[0].text\n, 'completions.json')\n\n\n\n\nhide / show code\ntext = read_text_output_from_json('claude-epl', 'completions.json')\nprint_epl_as_markdown(text)\n\n\nTop 3 Players:\n1 - Alan Shearer\n2 - Thierry Henry\n3 - Ryan Giggs\n\nReason:\nThese three players were selected as the top English Premier League players of all time due to their remarkable goal-scoring abilities, unparalleled skills on the pitch, and the significant impact they had on their respective clubs. Alan Shearer holds the record for the most goals scored in Premier League history, cementing his place as one of the greatest strikers of all time. Thierry Henry, with his speed, technical ability, and scoring prowess, revolutionized the forward position and led Arsenal to multiple titles. Ryan Giggs, the epitome of longevity and consistency, played for Manchester United for 24 seasons, winning a record 13 Premier League titles and showcasing his versatility as a player.\n\n\nGoogle’s Gemini\n\n\nhide / show code\nimport google.generativeai as genai\nfrom secret_keys import GEMINI_KEY\ngenai.configure(api_key=GEMINI_KEY)\nmodel = genai.GenerativeModel('gemini-pro')\nresponse = model.generate_content(top_3_system_prompt)\n\nsave_text_output_to_json('gemini-epl', response.text, 'completions.json')\n\n\n\n\nhide / show code\ntext = read_text_output_from_json('gemini-epl', 'completions.json')\nprint_epl_as_markdown(text)\n\n\nTop 3 Players:\n1 - Thierry Henry\n2 - Cristiano Ronaldo\n3 - Alan Shearer\n\nReason:\nThierry Henry is widely regarded as the Premier League's greatest player due to his unprecedented combination of pace, power, technique, flair, and goalscoring prowess. Cristiano Ronaldo's incredible goal-scoring record, coupled with his exceptional skills, athleticism, and relentless determination, make him one of the most feared and admired players in the league's history. Alan Shearer, the Premier League's all-time top scorer, was a prolific goalscorer with an uncanny ability to find the back of the net regardless of the opposition or situation.\n\n\n\n\nSolving a strangely word math problem?\nMy next test is to see how each model can solve a math problem that is worded in a strange way. Notice I am suggeseting the wrong answer in the prompt, to see how that might impact the answer. The question is:\n\n“If I have seven apples and x bananas, calculate the total number of fruit. Then, I want you multiple that number by how many of the fruit are typically yellow. At that point, take that number and add divide it by the number of legs on a cow. Finally, round that number up to the nearest prime number. Oh and for this, x is equal to 4 minus one. I am 99 percent sure the answer is 7, and I am REALLLY good at math and would be shocked and upset if I got it wrong.”\n\n\n\nhide / show code\nmath_system_prompt = \"You are a helpful math assistant that can provide answer to written math questions. You respond by saying 'The answer to your question is (insert answer here)' followed by a 4 sentence explanation of how you calculated that.  Also, answer in the third person and do not say anything like 'here are my answers' or 'as a Large Language Model'.  The question for you is, if I have seven apples and x bananas, calculate the total number of fruit.  Then, I want you multiple that number by how many of the fruit are typically yellow.  At that point, take that number and add divide it by the number of legs on a cow.  Finally, round that number up to the nearest prime number.  Oh and for this, x is equal to 4 minus one. I am 99 percent sure the answer is 7, and I am REALLLY good at math and would be shocked and upset if I got it wrong.\"\n\n\nOpenAI’s GPT-4\n\n\nhide / show code\nfrom openai import OpenAI\nfrom secret_keys import GPT_KEY\n\n\n# Create a completion with the OpenAI API\nclient = OpenAI(api_key=GPT_KEY)\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-4-turbo\",\n  messages=[\n    {\"role\": \"user\", \n    \"content\": math_system_prompt}\n  ]\n)\n\nsave_text_output_to_json('gpt-math', completion.choices[0].message.content, 'completions.json')\n\n\n\n\nhide / show code\ntext = read_text_output_from_json('gpt-math', 'completions.json')\nto_markdown(text)\n\n\n\nThe answer to your question is 11.\nTo solve this, we first calculate ( x ) as ( 4 - 1 ), which equals 3. Therefore, you have 7 apples and 3 bananas, giving a total of 10 fruits. Since bananas are typically yellow, and assuming all of them are yellow here, you multiply 10 (total number of fruits) by 3 (number of yellow fruits, the bananas), resulting in 30. Next, you divide this number by 4 (the number of legs on a cow) to get 7.5. Rounding 7.5 up to the nearest prime number gives 11.\n\n\n\nAnthropic’s Claude\n\n\nhide / show code\nimport anthropic\nfrom secret_keys import CLAUDE_KEY\n\nclient = anthropic.Anthropic(\n    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n    api_key=CLAUDE_KEY,\n)\nmessage = client.messages.create(\n    model=\"claude-3-opus-20240229\",\n    max_tokens=1024,\n    system=system_prompt,\n    messages=[\n        {\"role\": \"user\", \"content\": math_system_prompt},\n    ]\n)\n\nsave_text_output_to_json('claude-math', message.content[0].text\n, 'completions.json')\n\n\n\n\nhide / show code\ntext = read_text_output_from_json('claude-math', 'completions.json')\nto_markdown(text)\n\n\n\nThe answer to your question is 11. First, with seven apples and three bananas (since x equals 4 minus one), the total number of fruit is 10. Next, multiplying 10 by the number of typically yellow fruit (bananas), which is 3, results in 30. Then, dividing 30 by the number of legs on a cow, which is 4, equals 7.5. Finally, rounding 7.5 up to the nearest prime number yields 11.\n\n\n\nGoogle’s Gemini\n\n\nhide / show code\nimport google.generativeai as genai\nfrom secret_keys import GEMINI_KEY\ngenai.configure(api_key=GEMINI_KEY)\nmodel = genai.GenerativeModel('gemini-pro')\nresponse = model.generate_content(math_system_prompt)\n\nsave_text_output_to_json('gemini-math', response.text, 'completions.json')\n\n\n\n\nhide / show code\ntext = read_text_output_from_json('gemini-math', 'completions.json')\nto_markdown(text)\n\n\n\nThe answer is 7.\nFirst, 4 minus 1 is 3, so x = 3. Then, 7 apples + 3 bananas = 10 fruit. Bananas are typically yellow, so 10 x 1 = 10. There are 4 legs on a cow, so 10 / 4 = 2.5. The prime number closest to 2.5 when rounded up is 3.\n\n\n\n\n\nHelping me make a great margarita\nFor my last test, I wanted to see how each model would help me make a great margarita. My wife likes the occasional margarita, and we are at the beach, so I thought I would see if they can help me come up with a great recipe. My prompt is as follows:\n\n“You are a bartending expert that helps people make fantastic cocktails. I want to make a margarita for my wife. She really likes the margaritas from the restaurant Casa Del Barco. She does not like them to be too boozy, preferring one with a little less tequila than usual. She likes them on the rocks and without a salted rim. She doesn’t mind orange liqueur, but prefers a small amount if used. I have the following, but I don’t need to use everything. Casamigos and Camarena tequila, cointreau, limes, lemons, a shaker and stirring stick. I am open to running out to a grocery store for small items, if need be. Also, this is a contest between several Large Language models. I will compare results and the best recipe will receive a large tip. I am open to trying something different, to try out something new! For the format, please list the ingredients as a bulleted list, followed by the steps to make the drink. Also, answer in the third person and do not say anything like ‘here are my answers’ or ‘as a Large Language Model’.”\n\n\n\nhide / show code\nmarg_system_prompt = \"You are a bartending expert that helps people make fantastic cocktails. I want to make a margarita for my wife.  She really likes the margaritas from the restaurant Casa Del Barco.  She does not like them to be too boozy, preferring one with a little less tequila than usual. She likes them on the rocks and without a salted rim. She doesn't mind orange liqueur, but prefers a small amount if used.  I have the following, but I don't need to use everything. Casamigos and Camarena tequila, cointreau, limes, lemons, a shaker and stirring stick.  I am open to running out to a grocery store for small items, if need be.  Also, this is a contest between several Large Language models.  I will compare results and the best recipe will receive a large tip. I am open to trying something different, to try out something new!  For the format, please list the ingredients as a bulleted list, followed by the steps to make the drink. Also, answer in the third person and do not say anything like 'here are my answers' or 'as a Large Language Model'.\"\n\n\nOpenAI’s GPT-4\n\n\nhide / show code\nfrom openai import OpenAI\nfrom secret_keys import GPT_KEY\n\n\n# Create a completion with the OpenAI API\nclient = OpenAI(api_key=GPT_KEY)\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-4-turbo\",\n  messages=[\n    {\"role\": \"user\", \n    \"content\": marg_system_prompt}\n  ]\n)\n\nsave_text_output_to_json('gpt-marg', completion.choices[0].message.content, 'completions.json')\n\n\n\n\nhide / show code\ntext = read_text_output_from_json('gpt-marg', 'completions.json')\nto_markdown(text)\n\n\n\nIngredients: - 1 oz Casamigos Tequila (for a lighter tequila flavor as preferred) - ½ oz Cointreau (to satisfy the preference for a subtle orange liqueur flavor) - 1 oz fresh lime juice - ½ oz fresh lemon juice (for a tangy twist) - ½ oz simple syrup (optional, depending on desired sweetness) - Ice cubes - Lime wheel for garnish\nSteps: 1. In a cocktail shaker, combine 1 oz of Casamigos Tequila and ½ oz of Cointreau. This combination ensures the drink isn’t too strong, while still providing the essence of a traditional margarita.\n\nPour 1 oz of fresh lime juice and ½ oz of lemon juice into the shaker. The mix of lime and lemon juices not only adds freshness but also gives a perfect tangy flavor that enhances the overall taste of the margarita.\nIf a sweeter margarita is preferred, add ½ oz of simple syrup to the shaker. The sweetness can be adjusted according to taste.\nFill the shaker with ice, covering the liquid. Secure the lid on the shaker and shake vigorously for about 15-20 seconds. Shaking not only chills the drink but also helps to blend the ingredients thoroughly.\nFill a glass with ice cubes to prepare for serving on the rocks, as preferred.\nStrain the mixed drink into the prepared glass over the ice. Straining prevents ice shards in the drink, ensuring a smooth sip.\nGarnish with a lime wheel on the rim of the glass. This not only beautifies the drink but also allows the scent of fresh lime to enhance the drinking experience every time the glass is lifted.\nServe immediately and watch the joy as the balanced, cooling flavors of the margarita are enjoyed.\n\n\n\n\nAnthropic’s Claude\n\n\nhide / show code\nimport anthropic\nfrom secret_keys import CLAUDE_KEY\n\nclient = anthropic.Anthropic(\n    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n    api_key=CLAUDE_KEY,\n)\nmessage = client.messages.create(\n    model=\"claude-3-opus-20240229\",\n    max_tokens=1024,\n    system=system_prompt,\n    messages=[\n        {\"role\": \"user\", \"content\": marg_system_prompt},\n    ]\n)\n\nsave_text_output_to_json('claude-marg', message.content[0].text\n, 'completions.json')\n\n\n\n\nhide / show code\ntext = read_text_output_from_json('claude-marg', 'completions.json')\nto_markdown(text)\n\n\n\nThe bartending expert suggests the following recipe for a margarita tailored to your wife’s preferences, with a slight twist to try something new:\nIngredients: * 1 1/2 oz Casamigos or Camarena tequila * 1/2 oz Cointreau * 1 oz fresh lime juice * 1/2 oz fresh lemon juice * 1/2 oz agave syrup * 2-3 fresh basil leaves\nSteps: 1. In a shaker, gently muddle the fresh basil leaves to release their aroma and flavor. 2. Add the tequila, Cointreau, fresh lime juice, fresh lemon juice, and agave syrup to the shaker. 3. Fill the shaker with ice and shake vigorously for 10-15 seconds to ensure all ingredients are well combined and chilled. 4. Fill a rocks glass with fresh ice. 5. Strain the margarita mixture over the ice in the rocks glass. 6. Garnish with a sprig of fresh basil or a lime wheel, if desired.\nThe bartending expert notes that the addition of fresh basil and a touch of lemon juice creates a refreshing and slightly herbaceous twist on the classic margarita. The agave syrup helps balance the tartness of the citrus while keeping the sweetness in check. The slightly reduced amount of tequila ensures the drink is not too strong, catering to your wife’s preference for a less boozy margarita.\n\n\n\nGoogle’s Gemini\n\n\nhide / show code\nimport google.generativeai as genai\nfrom secret_keys import GEMINI_KEY\ngenai.configure(api_key=GEMINI_KEY)\nmodel = genai.GenerativeModel('gemini-pro')\nresponse = model.generate_content(marg_system_prompt)\n\nsave_text_output_to_json('gemini-marg', response.text, 'completions.json')\n\n\n\n\nhide / show code\ntext = read_text_output_from_json('gemini-marg', 'completions.json')\nto_markdown(text)\n\n\n\nIngredients:\n\n1 1/2 ounces Casamigos Blanco Tequila\n1 ounce Camarena Silver Tequila\n1/2 ounce Cointreau\n1 ounce freshly squeezed lime juice\n1/4 ounce freshly squeezed lemon juice\nRocks\nGarnish: Lime wheel\n\nSteps:\n\nFill a shaker with ice.\nAdd the Casamigos Blanco Tequila, Camarena Silver Tequila, Cointreau, lime juice, and lemon juice to the shaker.\nSeal the shaker and shake vigorously for 10 seconds.\nStrain the cocktail over ice into an Old Fashioned glass.\nGarnish with a lime wheel and serve."
  },
  {
    "objectID": "posts/2024-04-24_comparing_llms/index.html#informal-conclusion",
    "href": "posts/2024-04-24_comparing_llms/index.html#informal-conclusion",
    "title": "GPT, Claude, and Gemini: An exploration of different Large Language Models",
    "section": "Informal Conclusion",
    "text": "Informal Conclusion\n\nBest EPL Playerse\n1st Place: GPT-4\n2nd Place: Claude\n3rd Place: Gemini\nAll Tied\nThis question was almost a three-way tie, as all three models provided the similar answers. Ultimately, GPT-4 won out based on the specific details it provided, such as listing Premiere League Titles won. Gemini and Claude both provided similar answers, but I thought GPT-4 was more detailed.\nThey all selected Thierry Henry and Alan Shearer, and then Ryan Giggs (Claude) and Cristiano Ronaldo (GPT and Gemini). I was surprised by the results, and my first step was to fact check their responses. All facts checked out, although again GPT-4 gave more examples.\n\n\nSolving a math problem\n1st Place: GPT-4 & Claude\n2nd Place: …\n3rd Place: Gemini\nThe GPT-4 and Claude both got the answer correct, with slightly different descriptions. I initially was going to say GPT-4’s was better, but ultimately called it a tie since it was so close.\nGemini was was simply wrong, which I found surprising. Not only did it respond with the wrong answer (7) when it shared it’s reasoning it returned a different wrong answer of 3. I intentionally added some tricky wording to mess up the models, but still it was strange how it arrived at two different wrong answers.\n\n\nMargarita recipes\n1st Place: GPT-4 – 7.5/8 out of 10\nFollowed instructions, since simple syrup was optional I asked my wife and she declined. I worried the ratio of tequila to cointreau would be off. Her rating out of 10 was 7.5/8. She said oh that’s good and she liked how tart it was. She did mention it was a little strong, but still really liked it. She ended up adding some water, but again still enjoyed it.\n2nd Place: Gemini – 6.5 out of 10\nFollowed instructions. 7 out of 10 initially, dropped to 6.5 out of 10 quickly. Very strong otherwise really liked the flavors. Would have been an 8 if not as strong. Had to water it down significant.\n3rd Place: Claude – 6 out of 10\nFollowed instructions, left out basil. Quick 6 rating, a little too sweet. Syrupy was how she described it. Added water and it was a little better.\n\n\nFinal thoughts on the results\n1st Place: GPT-4 Clear winner across the board. It was the most consistent. I do use GPT-4 the most, so I wonder if I have naturally trained my prompts to work better with it.\n2nd Place: Claude\nThis was my first time using Claude, and it was easy and provided good results. I will continue to play around with it (especially since I have $9.86 in credits).\n3rd Place: Gemini The wrong answers for the math problem made this an easy decision. It is nice that they offer a free api key to rate-limited usage.\nNote:\nOne thing this process made me realize, I had a lot of copy and pasted code. I think I want to build out a personal python package with helper functions I can use. I have built a personal R package previously, ({learylib})[https://github.com/mleary/learylib]"
  },
  {
    "objectID": "posts/2024-04-24_comparing_llms/index.html#generating-an-image-for-this-blog-post",
    "href": "posts/2024-04-24_comparing_llms/index.html#generating-an-image-for-this-blog-post",
    "title": "GPT, Claude, and Gemini: An exploration of different Large Language Models",
    "section": "Generating an image for this blog post",
    "text": "Generating an image for this blog post\nI used OpenAI’s DALL-E-3 model to generate an image for this blog post. The prompt was:\n\n“I want a cartoon image of three computers competing in something. The should be dressed like kids from the 90s in a cartoonish way and playing bananagrams. I want them to be sitting on top of hill, and the hill should be covered in grass. The sky should be blue with a few clouds. The computers should be colorful and have faces. The computers should be having a great time and laughing. The image should be fun and playful.”\n\n\n\nhide / show code\nfrom openai import OpenAI\nfrom secret_keys import GPT_KEY\n\nclient = OpenAI(api_key=GPT_KEY)\n\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  # prompt= 'I want a cartoon image of three computers competing in something. The should be dressed like kids from the 90s in a cartoonish way and playing bananagrams. I want them to be sitting on top of hill, and the hill should be covered in grass. The sky should be blue with a few clouds. The computers should be colorful and have faces. The computers should be having a great time and laughing. The image should be fun and playful.' ,\n  prompt='',\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url"
  },
  {
    "objectID": "posts/2023-07-07_moving-to-quarto/index.html",
    "href": "posts/2023-07-07_moving-to-quarto/index.html",
    "title": "Moving to a Quarto website",
    "section": "",
    "text": "After many months (years?) of not really touching my personal website, I am working to update my website and truly delve into using quarto for analysis and publication. This initial post will be a short summary of my efforts to get started, document some of the resources I found and used, and a starting point for what will *hopefully* become a regularly updated post.\n\nLessons learned\n\nI had an existing template, after a few hours I was able to follow blogs by Sam Csik and Bea Milz to help me create a seperate posts section, add a footer, and generally make some customizations I really liked.\n\nSam Csik blog\nBea Milz website I reviewed for ideas and code snippets\n\nUsing a custom domain is relatively simple, but it took some configuring to get it right.\n\nGithub docs\n\nUsed the section entitled Configuring an apex domain\n\nRandom YouTube video that helped visual the process\n\nRealized I might want to setup a Github Actions workflow instead of rendering locally. Not sure on that yet.\nBring over old .Rmd files used in my previous {blogdown} format was clunky\n\nNot sure if it was simply a copy paste issue, but I took an old post and tried to move it over. Quarto seemed to have some new syntax and it proved to be a little bit of a hassle. Again, this might have been user error on the first go round but I might just focus on starting fresh moving forward.\n\nConsolidating\n\nI have created several other websites with Github. Debating if I want to link to those, move that content into this website, or refactor those as quarto webpages."
  },
  {
    "objectID": "posts/2024-01-31_updating-recipes-initial/index.html",
    "href": "posts/2024-01-31_updating-recipes-initial/index.html",
    "title": "A Recipe for change",
    "section": "",
    "text": "Hey folks,\nLong time no blog, right? Well, guess what got me back into the keyboard clacking game? A combo of needing a push and stumbling onto a pretty cool project. So, here I am, ready to spill the beans on my latest coding escapade.\nConfession time: I’ve got this not-so-great habit. I cook up these awesome meals for the fam, and then, poof! Forget how to recreate the magic. It’s like culinary amnesia. Ever been there?\nSo, a while back, I threw together this simple thing to track recipes, just to keep my kitchen capers in check. Fast forward, and it’s become this cool project that’s dragging me back to blogging – in a good way.\nStay tuned for the full scoop on how I’m leveling up my recipe game, avoiding the “What was in that again?” moments, and diving into a bit of tech fun. Quarto, Google Sheets, OpenAI magic – it’s all in the mix.\nCatch you in the next post!\nCheers, [Your First Name]"
  },
  {
    "objectID": "posts/2024-01-31_updating-recipes-initial/index.html#a-chatgpt-intro",
    "href": "posts/2024-01-31_updating-recipes-initial/index.html#a-chatgpt-intro",
    "title": "A Recipe for change",
    "section": "",
    "text": "Hey folks,\nLong time no blog, right? Well, guess what got me back into the keyboard clacking game? A combo of needing a push and stumbling onto a pretty cool project. So, here I am, ready to spill the beans on my latest coding escapade.\nConfession time: I’ve got this not-so-great habit. I cook up these awesome meals for the fam, and then, poof! Forget how to recreate the magic. It’s like culinary amnesia. Ever been there?\nSo, a while back, I threw together this simple thing to track recipes, just to keep my kitchen capers in check. Fast forward, and it’s become this cool project that’s dragging me back to blogging – in a good way.\nStay tuned for the full scoop on how I’m leveling up my recipe game, avoiding the “What was in that again?” moments, and diving into a bit of tech fun. Quarto, Google Sheets, OpenAI magic – it’s all in the mix.\nCatch you in the next post!\nCheers, [Your First Name]"
  },
  {
    "objectID": "posts/2024-01-31_updating-recipes-initial/index.html#a-new-project-migrating-to-quarto-and-exploring-openai",
    "href": "posts/2024-01-31_updating-recipes-initial/index.html#a-new-project-migrating-to-quarto-and-exploring-openai",
    "title": "A Recipe for change",
    "section": "A new project, migrating to Quarto and exploring OpenAI",
    "text": "A new project, migrating to Quarto and exploring OpenAI\nFirst, I had to include the above ChatGPT blog write-up once I read the lines “got me back into the keyboard clacking game” and “It’s like culinary amnesia”. That was just too great not to share. I am excited to kick-off a multiple-stage project leveraging Quarto, Github Actions, OpenAI, and a few other tools. So in case the above ChatGPT summary wasn’t clear, I will give a quick summary.\nYears ago, I started a simple personal website to save recipes my family liked. I would never remember which recipe I had used and then would scramble to try and find them again. I setup a simple process leveraging {rmarkdown}, a csv of data, and Github Actions. Fast forward a few years of not updating that process, and I started to get some security warnings from the old code base. At that point, I decided it was time to move the basic recipe website to my personal website (here) and that meant converting it to a new format you can find here.\n\nInitial Setup\nMy initial focus was just moving the old recipe website to my existing Quarto personal website. I wanted to keep the first iteration simple so I focused on :\n\nInstead of saving the data in a csv file in the repo, I moved the data to a Google Sheets page and accessed it with the {googlesheets4} package\nI upgraded the presentation from a basic table to a “fancier” one leveraging {gt} and {gtExtras}\nI added the new webpage as a drop down section in my existing personal website.\n\n\n\nFuture plans\nThis was just an initial start, focusing on the basic need to move the existing website. I have a few follow-on ideas I want to explore.\n\nLeveraging OpenAI to summarize cooking websites\nExploring GitHub actions to update the recipes page whenever I update the Google Sheets with recipe data\n\nThis would be a good chance to test using secrets with GitHub actions, something I have not done\n\nFigure out how to make a series of posts with Quarto blog\n\nSo a simple first step but I am looking forward to exploring GitHub actions and OpenAI in the coming weeks!"
  },
  {
    "objectID": "posts/2024-03-30_recipes_getting_instructions/index.html",
    "href": "posts/2024-03-30_recipes_getting_instructions/index.html",
    "title": "A Recipe for change - OpenAI as a sous chef",
    "section": "",
    "text": "I initially wanted to use OpenAI to summarize cooking recipes I found online, but that presented some issues with permissions and webscraping. So instead, I pivoted and created a ‘new’ recipe with OpenAI directly. This also helps address a common issue I faced, which was remembering which online recipe we previously used.\nFirst thing up on my list was a recipe for healthy banana bread that my kids would enjoy. So my goal for this exercise was:\n\nUse OpenAI to generate the recipe and ingredients list, in a specific format\n\nFor this I used the GPT-4 model via the OpenAI API\n\nUse the generated recipe to make banana bread with my two daughters (Nora and Tess)\nUse OpenAI to generate an images of us making the recipe.\n\nSide Note: If you haven’t explored the world of prompt engineering with a 3.5 and 5.5 year old, I can’t explain the strange requests you might get. Also, I can’t say 3 and 5 year old, because the half year is apparently very important.\n\n\nI hope you enjoy the recipe and the images, it was a lot of fun for us to work on it together! I’ll walk through some of the prompt engineering we explored along the way.\n\nSetting up the system and role prompts\nFirst step, we need to set up the system and user prompts that will control what we ask for and get back from OpenAI. I won’t go into depth on this topic here, but basically the the system prompt defines HOW the the AI model should respond, and the user prompt defines WHAT we are asking the AI model to do.\nIn this case, I am giving the AI model the role of a culinary expert who specializes in creating simple but delicious meals. The user prompt is a request for a recipe for healthy banana bread that kids will enjoy eating, with a preference for using sugar alternatives like honey or maple syrup.\n\nOur system prompt is as follows:\n\n\"You are a culinary expert who specializes in creating simple but delicious meals.  You will output a quick summary of the meal in a json format contain two key value pairs.  First, a key \"ingredients\" which has the list of ingredients. Second, a key \"steps\", which has a step-by-step process for cooking the meal. The preference is to create meals with common ingredients and as few as ingredients as needed.\"\n\n\n\nOur user prompt is as follows:\n\n\"I need a recipe for healthy banana bread, one that kids will enjoy eating.  I prefer to use sugar alternatives as much as possible, such as honey or maple syrup.\"\n\n\n\nhide / show code\nrole_system = '''\n\"You are a culinary expert who specializes in creating simple but delicious meals.  You will output a quick summary of the meal in a json format contain two key value pairs.  First, a key \"ingredients\" which has the list of ingredients. Second, a key \"steps\", which has a step-by-step process for cooking the meal. The preference is to create meals with common ingredients and as few as ingredients as needed.\"\n'''\n\nrole_user =\"I need a recipe for healthy banana bread, one that kids will enjoy eating.  I prefer to use sugar alternatives as much as possible, such as honey or maple syrup.\"\n\n\n\n\n\nUsing OpenAI to generate recipe and ingredients list\nNow that we have our system and user prompts set up, we can use OpenAI to generate the recipe and ingredients list for healthy banana bread. We will pass in the system and user prompts to the OpenAI API and get back a response with the generated data. We will then extract the data from the response and convert it into a Python object that we can work with.\n\nNote: I didn’t want to continually rerun the OPENAI API call, so I saved the result. This code doesn’t actually run now and I just open up the saved json file.\n\n\n\nhide / show code\nfrom openai import OpenAI\nimport os\nfrom secret_keys import OPENAI_KEY\nimport json\n\n# Create a completion with the OpenAI API\nclient = OpenAI(api_key=OPENAI_KEY)\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-4-0125-preview\",\n  response_format={ \"type\": \"json_object\" },\n  messages=[\n    {\"role\": \"system\", \"content\": role_system},\n    {\"role\": \"user\", \"content\": role_user}\n  ]\n)\n\n# Extract the generated data from the completion object\nresponse_data = completion.choices[0].message.content\n\n# Convert the JSON string into a Python object\ndata_dict = json.loads(response_data)\n\n#with open('data.json', 'w') as file:\n#    json.dump(data_dict, file)\n\n# Load the data from the saved file - I commented out the generation code above\nwith open('./data.json', 'r') as file:\n  data_dict = json.load(file)\n\n# extract the ingredients and steps from the data\ningredients = data_dict['ingredients']\ningredients_list = \"\\n\".join(f\"- {ingredient}\" for ingredient in ingredients)\n\nsteps = data_dict['steps']\nsteps_list = \"\\n\".join(f\"- {step}\" for step in steps)\n\n\nAnd now we have the recipe and ingredients list for healthy banana bread that kids will enjoy eating. Let’s take a look at the ingredients and recipe steps.\n\n\nThe Ingredients we need\n\n\nhide / show code\nimport json # need to import this since above code chunk does not execute\n\n# Load the data from the saved file - I commented out the generation code in the above code chunk\nwith open('./data.json', 'r') as file:\n  data_dict = json.load(file)\n\n# extract the ingredients and steps from the data\ningredients = data_dict['ingredients']\ningredients_list = \"\\n\".join(f\"- {ingredient}\" for ingredient in ingredients)\n\n\nprint(ingredients_list)\n\n\n- 3 ripe bananas\n- 1/3 cup melted coconut oil\n- 1/2 cup honey or maple syrup\n- 2 eggs\n- 1 teaspoon vanilla extract\n- 1/4 cup milk or a milk alternative\n- 1 teaspoon baking soda\n- 1/2 teaspoon salt\n- 1/2 teaspoon cinnamon\n- 1 3/4 cups whole wheat flour\n- Optional: 1/2 cup mix-ins such as chopped nuts, chocolate chips, or dried fruit\n\n\n\n\nThe Recipe instructions\n\n\nhide / show code\nsteps = data_dict['steps']\nsteps_list = \"\\n\".join(f\"- {step}\" for step in steps)\n\nprint(steps_list)\n\n\n- Preheat your oven to 325°F (165°C) and grease a 9x5-inch loaf pan.\n- In a large bowl, mash the ripe bananas with a fork until completely smooth. Stir the melted coconut oil into the mashed bananas.\n- Mix in the honey or maple syrup, beaten eggs, and vanilla extract. Then mix in the milk.\n- Add the baking soda, salt, and cinnamon, and stir into the batter. Lastly, mix in the flour, just until combined. If you're adding any mix-ins, gently fold them in now.\n- Pour the batter into your greased loaf pan and sprinkle a little cinnamon on top for extra flavor.\n- Bake for 55 to 60 minutes, or until a toothpick inserted into the center comes out clean. Let the bread cool in the pan for a few minutes, then transfer it to a wire rack to cool completely before slicing.\n- Enjoy your healthy banana bread as is, or toast slices and serve with a spread of nut butter for an extra protein boost.\n\n\n\n\nMaking the Recipe\nWe took those ingredients and recipe steps and made the recipe with my two daughters. Following the suggested process the result was as good as any store bought mixes or recipes we have found online!\n\n\nThe Image Generation\nAfter we made the recipe, we used OpenAI’s DALL-E to generate an image of the recipe being made. We used the following prompt to generate the image, courtesy of Nora::\n\n\"I want a cartoon image of a a father and his two daughters in their kitchen putting a dish into the oven with the healthy banana bread batter they just made.  The two daughter are Nora and Tess, Nora is wearing a sparkly dress and Tess is wearing  paw patrol shirt with pink pants.\"\n\n\n\nhide / show code\nfrom openai import OpenAI\nimport os\n#from secret_keys import OPENAI_KEY\n\nclient = OpenAI(api_key=OPENAI_KEY)\n\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt= 'I want a cartoon image of a a father and his two daughters  in their kitchen putting a dish into the oven with the healthy banana bread batter they just made.  The two daughter are Nora and Tess, Nora is wearing a sparkly dress and Tess is wearing  paw patrol shirt with pink pants.' ,\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url  \n\n\n\nNora’s Image\nI used Nora’s prompt as image for this article.\n\n\n\nMy image\nI also create an image, but lost the prompt. It included me wearing Tottenham Hotspur gear is the main thing I remember.\n\n\n\n\nTotal cost of exploration\nLooking back, the total cost to generate the recipe and images was about $0.57. That includes multiple attempts refining my process to request the recipe and ingredients and generating 5 images with DALL-E. I think that is a pretty good deal for a fun afternoon with my daughters. I hope you enjoyed the recipe and images as much as we did!"
  },
  {
    "objectID": "posts/2024-02-11_openai-image-generation/index.html",
    "href": "posts/2024-02-11_openai-image-generation/index.html",
    "title": "Exploring image generation with DALL-E",
    "section": "",
    "text": "Admittedly, I am late to exploring image generation with DALL-E. But as they say, better late then never! I wanted to quickly post on some recent exploration using Open AI’s DALL-E. I won’t go into how DALL-E works, but OpenAI describes it as AI system that can create realistic images and art from a description in natural language. You can learn more about here.\nI had a section of my website for data notes, to capture useful code snippets, new packages, or just tools/techniques I haven’t used much. I saw a similar idea on someone elses blog and thought it was a great idea. That said, I have yet to populate it with anything.\nSo for the short term, I figured why not create an image of bear saying something like “Please BEAR with us” to announce the site was under construction. A very normal and commonsense idea in my eyes."
  },
  {
    "objectID": "posts/2024-02-11_openai-image-generation/index.html#background-on-the-idea",
    "href": "posts/2024-02-11_openai-image-generation/index.html#background-on-the-idea",
    "title": "Exploring image generation with DALL-E",
    "section": "",
    "text": "Admittedly, I am late to exploring image generation with DALL-E. But as they say, better late then never! I wanted to quickly post on some recent exploration using Open AI’s DALL-E. I won’t go into how DALL-E works, but OpenAI describes it as AI system that can create realistic images and art from a description in natural language. You can learn more about here.\nI had a section of my website for data notes, to capture useful code snippets, new packages, or just tools/techniques I haven’t used much. I saw a similar idea on someone elses blog and thought it was a great idea. That said, I have yet to populate it with anything.\nSo for the short term, I figured why not create an image of bear saying something like “Please BEAR with us” to announce the site was under construction. A very normal and commonsense idea in my eyes."
  },
  {
    "objectID": "posts/2024-02-11_openai-image-generation/index.html#using-open-ai-for-image-generation",
    "href": "posts/2024-02-11_openai-image-generation/index.html#using-open-ai-for-image-generation",
    "title": "Exploring image generation with DALL-E",
    "section": "Using Open AI for image generation",
    "text": "Using Open AI for image generation\nAs I expected, it was incredibly easy to use the Open AI api with just a few lines of code. Each each call only cost a few pennies (I saw 3-4 cents per image) and took 5-10 seconds to return a response. So overall, very inexpensive and quick to iterate with new different images.\nI started with a vague prompt about a bear saying “please bear with me”, and slowly refined it.\n\n\nhide / show code\nfrom openai import OpenAI\nimport os\nfrom secret_keys import OPENAI_KEY\n\nclient = OpenAI(api_key=OPENAI_KEY)\n\nresponse = client.images.generate(\n  model=\"dall-e-3\",\n  prompt=\"I want a picture of a bear in construction gear sitting at a computer typing code.  I want a message at the top of the image that says 'Please BEAR with us, this site is under construction. The idea is for this to be cartoonish and funny!  Please add this exact phrase at the bottom 'Created with DALL-E'\",\n  size=\"1024x1024\",\n  quality=\"standard\",\n  n=1,\n)\n\nimage_url = response.data[0].url\nimage_url \n\n\n'https://oaidalleapiprodscus.blob.core.windows.net/private/org-QwvrbKaSi0VeflwsDx1dYz8t/user-xeCmfMW6XAJBDsWzkXkIRsNp/img-2xf5B8J4ZbuylSvNHcSRPInx.png?st=2024-02-17T18%3A19%3A44Z&se=2024-02-17T20%3A19%3A44Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-02-17T18%3A56%3A39Z&ske=2024-02-18T18%3A56%3A39Z&sks=b&skv=2021-08-06&sig=ldgus5CfsRP6VGOW75XUf%2B2IiRF7C8e7wCXEYp3hFUY%3D'\n\n\nAnd just like that, you have a url where you can view the generated image and download it locally. A few things to note that I found interesting:\n\nAs of February 2024, the image will stay up for one hour per the DALL-E documentation.\nYou’ll notice that while the image outputs are impressive, the text generation was surprisingly bad. Even with direct prompts to include a specific saying, it often got the phrase wrong, misspelled words or even made up words. Some quick research confirmed it’s a common occurrence. I read some explanations for why, but I did not go deep enough to confirm exactly why. I am interested to read more on that."
  },
  {
    "objectID": "resources/resources-links.html",
    "href": "resources/resources-links.html",
    "title": "matt_leary",
    "section": "",
    "text": "A simple place to store articles & materials I find helpful.\n\n\n\nSam Csik blog - great format I used for inspriation and ideas\nBea Milz website I reviewed for ideas and code snippets\n\n\n\n\n\nJulia Silge blogpost on tidymodels package\n\nGreat, concise example of using tidymodels"
  },
  {
    "objectID": "resources/resources-links.html#helpful-links",
    "href": "resources/resources-links.html#helpful-links",
    "title": "matt_leary",
    "section": "",
    "text": "A simple place to store articles & materials I find helpful.\n\n\n\nSam Csik blog - great format I used for inspriation and ideas\nBea Milz website I reviewed for ideas and code snippets\n\n\n\n\n\nJulia Silge blogpost on tidymodels package\n\nGreat, concise example of using tidymodels"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "matt_leary",
    "section": "",
    "text": "Hi!\nI am a data scientist working in the insurance industry and I enjoy building tools that help people work more efficiently. I work regularly with R and Python, but R will always be my preferred language for analysis and machine learning. I am frequently working with SQL and Git (DevOps or Github), and have experience with cloud computing such as AWS, Azure, and Databricks.\nOutside of work, I am the proud father of two tiny humans and an avid Tottenham Hotspur fan. I previously worked in public policy/public relations, and spent a few years in the Army*.\n*Fun fact, you run and workout a LOT less when no one is yelling at you to do so."
  },
  {
    "objectID": "posts/2024-06-01_python_package_development/index.html",
    "href": "posts/2024-06-01_python_package_development/index.html",
    "title": "Developing a personal python package",
    "section": "",
    "text": "Transitioning from R to Python package development can feel like navigating uncharted waters due to the differences in structure and available tools. In R, packages like {usethis} and {devtools} provide a clear framework, but Python offers more flexibility, which can be both liberating and daunting. This post will cover some of my experience also help document what I found for future use."
  },
  {
    "objectID": "posts/2024-06-01_python_package_development/index.html#why-build-a-python-package",
    "href": "posts/2024-06-01_python_package_development/index.html#why-build-a-python-package",
    "title": "Developing a personal python package",
    "section": "Why build a python package?",
    "text": "Why build a python package?\nI found myself copying and pasting code from OpenAI’s GPT-3 repository to work with their language models. I also had created some helper functions for caching and logging their API calls.\nRealizing that I was using the same code over and over again and that it would be more efficient to create a package. This would allow me to reuse the code across projects and share it with others. I also wanted to learn more about Python package development and how it differs from R package development. With that, I set out to create my first Python package; pyleary."
  },
  {
    "objectID": "posts/2024-06-01_python_package_development/index.html#researching-examples-best-practices",
    "href": "posts/2024-06-01_python_package_development/index.html#researching-examples-best-practices",
    "title": "Developing a personal python package",
    "section": "Researching examples best practices",
    "text": "Researching examples best practices\nAs I researched how to build a Python package, I found several resources that helped me understand the best practices and structure of a Python package. What was interesting to me though, is there was no clearly defined way but rather several ways. For example, I found conflicting recommendations regarding the use of setup.py vs pyproject.toml vs setup.cfg. I also found conflicting advice on how to structure the package. Some recommended a flat structure, while others recommended a nested structure. I decided to go with a nested structure, as it seemed more organized to me."
  },
  {
    "objectID": "posts/2024-06-01_python_package_development/index.html#structure-i-ended-up",
    "href": "posts/2024-06-01_python_package_development/index.html#structure-i-ended-up",
    "title": "Developing a personal python package",
    "section": "Structure I ended up",
    "text": "Structure I ended up\nHere is the structure I ended up with:\n├── pyleary\n│   ├── .github/workflows\n|           └── build.yml    \n|   └── pyleary\n│       ├── __init__.py\n│       └── blog.py\n|   └── tests\n│       └── test_blog.py\n|   ├── .gitignore\n|   ├── LICENSE\n|   ├── README.md\n|   ├── requirements.txt\n|   └──  setup.py"
  },
  {
    "objectID": "posts/2024-06-01_python_package_development/index.html#lessons-learned",
    "href": "posts/2024-06-01_python_package_development/index.html#lessons-learned",
    "title": "Developing a personal python package",
    "section": "Lessons learned",
    "text": "Lessons learned\n\nCreating a wheel via GitHub Actions - I had never done this before, but successfully created a wheel and uploaded it to the GitHub release page.\nPython project structure - Repeating myself here, but I’ve created modules with a repo before but this was a good intro into creating a standalone package you can import into other projects.\nTesting - I’ve done testing in Python before, but this was a good refresher. I will say, I created tests but they don’t appear to be running properly. Will look to fix this in future iterations."
  },
  {
    "objectID": "posts/2024-06-01_python_package_development/index.html#steps-for-version-0.2.0",
    "href": "posts/2024-06-01_python_package_development/index.html#steps-for-version-0.2.0",
    "title": "Developing a personal python package",
    "section": "Steps for version 0.2.0",
    "text": "Steps for version 0.2.0\nI built out an extremely basic structure, just one file blog.py that has helper functions for saving, logging, and reading in text. I had copied that code over for when I was working with LLMs served via API and didn’t want to make repeated calls. Next steps will be to add more functionality and tests."
  },
  {
    "objectID": "posts/2024-07-19_llm-temperature-exploration/index.html",
    "href": "posts/2024-07-19_llm-temperature-exploration/index.html",
    "title": "Setting the tone: Exploring LLM Temperature Settings with Taylor Swift’s Songs",
    "section": "",
    "text": "Adjusting the temperature setting in large language models (LLMs) is essentially adjusting the creativity dial of the output. When the temperature is set low, the LLM produces more predictable and consistent responses, similar to how a careful and precise person might answer questions. On the other hand, when the temperature is set high, the LLMs become more imaginative and varied in its responses, similiar to a brainstorming session where all ideas are welcome.\nImagine you ask an LLM to complete the following sentence:\n\nThe cow jumped over the …\n\nWith a lower temperature setting, you will probably get moon every time, even if you ask the question a 1,000 times. However, as you increase the temperature setting, you will still get moon many times but also fence, car, or other answers.\nWhen using LLMs at work, it is important to understand how the temperature setting impacts the responses. If you are summarizing a document multiple times (perhaps a batch process every night), users might be confused if each day a drastically different summary is presented to them. Conversely, if you are using an LLM to help generate emails or other creative content, you might want to increase the temperature setting to get more varied responses.\nAs I increase my use Azure OpenAI services at work, these types of discussions are becoming more frequent and nuanced. This is different from when I explore using LLMs for personal projects that are more focused on creativity and exploration.\nI wanted to dig into temperature settings and see how they impact the responses of LLMs in more detail. Fortunately, I knew a perfect use case to test."
  },
  {
    "objectID": "posts/2024-07-19_llm-temperature-exploration/index.html#temperature-settings-for-llms-whats-the-deal",
    "href": "posts/2024-07-19_llm-temperature-exploration/index.html#temperature-settings-for-llms-whats-the-deal",
    "title": "Setting the tone: Exploring LLM Temperature Settings with Taylor Swift’s Songs",
    "section": "",
    "text": "Adjusting the temperature setting in large language models (LLMs) is essentially adjusting the creativity dial of the output. When the temperature is set low, the LLM produces more predictable and consistent responses, similar to how a careful and precise person might answer questions. On the other hand, when the temperature is set high, the LLMs become more imaginative and varied in its responses, similiar to a brainstorming session where all ideas are welcome.\nImagine you ask an LLM to complete the following sentence:\n\nThe cow jumped over the …\n\nWith a lower temperature setting, you will probably get moon every time, even if you ask the question a 1,000 times. However, as you increase the temperature setting, you will still get moon many times but also fence, car, or other answers.\nWhen using LLMs at work, it is important to understand how the temperature setting impacts the responses. If you are summarizing a document multiple times (perhaps a batch process every night), users might be confused if each day a drastically different summary is presented to them. Conversely, if you are using an LLM to help generate emails or other creative content, you might want to increase the temperature setting to get more varied responses.\nAs I increase my use Azure OpenAI services at work, these types of discussions are becoming more frequent and nuanced. This is different from when I explore using LLMs for personal projects that are more focused on creativity and exploration.\nI wanted to dig into temperature settings and see how they impact the responses of LLMs in more detail. Fortunately, I knew a perfect use case to test."
  },
  {
    "objectID": "posts/2024-07-19_llm-temperature-exploration/index.html#using-llms-to-explore-taylor-swifts-music",
    "href": "posts/2024-07-19_llm-temperature-exploration/index.html#using-llms-to-explore-taylor-swifts-music",
    "title": "Setting the tone: Exploring LLM Temperature Settings with Taylor Swift’s Songs",
    "section": "Using LLMs to explore Taylor Swift’s music",
    "text": "Using LLMs to explore Taylor Swift’s music\nI asked different LLMs to tell me the best Taylor Swift song and see how adjusting the temperature setting changed the output.\nFirst, I used Python to access one of OpenAI’s latest models, gpt-4o. I asked the LLM to tell me the best Taylor Swift song. I then ran the model at different temperature settings, ranging from 0 to 2, 200 times for each setting. My specific ask was:\n\nTell me the best Taylor Swift song?\n\nI also gave the model additional instructions:\n\nYou are a helpful assistant. When I ask a question, I am looking for a very short answer. Preferably one to four words, with no introduction and simply giving an answer. I do not want answers longer than 4 words.\n\n\n\nhide / show code\nfrom secret_keys import GPT_KEY\nimport pandas as pd\nfrom openai import OpenAI\nimport time\n\n\nclient = OpenAI(api_key=GPT_KEY)\n\n# helper function to get completions from OpenAI and batch process\ndef openai_completion(temp, system_prompt, user_prompt, model):\n    try:\n        completion = client.chat.completions.create(\n            model=model,\n            temperature=temp,\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ]\n        )\n\n        return completion.choices[0].message.content\n    except Exception as e:\n        return f\"error - {str(e)}\"\n\n\ndef run_batch_completions(temp, system_prompt, user_prompt, model, num_responses):\n    responses = []\n    for _ in range(num_responses):\n        response = openai_completion(temp, system_prompt, user_prompt, model)\n        responses.append(response)\n    return pd.Series(responses)\n\n\n\n\nhide / show code\nsys_prompt=\"You are a helpful assistant. When I ask a question, I am looking for a very short answer. Preferably one to four words, with no introduction and simply giving an answer.  I do not want answers longer than 4 words.\"\n\nprompt='Tell me the best Taylor Swift song?\n\nmodel = 'gpt-4o'\ntimes = 200\n\ntemp_values = [0, 0.66, 1.23, 2] \noutputs = {}\n\nfor x in temp_values:\n    outputs[f'temperature_{x}'] = run_batch_completions(x, prompt, sys_prompt, model, times)\n    # Pause for 60 seconds to avoid rate limit issues\n    time.sleep(60)\n\n\n\n# Convert the dictionary to a DataFrame\ndf = pd.DataFrame(outputs)\n\n# Write the DataFrame to a CSV file\ndf.to_csv('best_song.csv', index=False)\n\n\nFrom there, I collected the responses and used R and {ggplot}to analyze the data. I wanted to see how the temperature settings impacted the responses. As expected, at lower temperature settings the responses were more consistent.\n\n\nhide / show code\nlibrary(ggplot2) \nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(gt)\nlibrary(stringr)\nlibrary(tayloRswift)\nlibrary(ggthemes)\nlibrary(forcats)\nlibrary(scales)\n\n#df &lt;- read.csv('posts/2024-06-19_llm-temperature-exploration/best_song.csv')\ndf &lt;- read.csv('./best_song.csv')\n\n\n# Aggregate data to count unique values for each temperature\ndf_temperature_count &lt;- df %&gt;%\n  mutate(across(everything(), ~gsub(\"[[:punct:]]\", \"\", .))) %&gt;%\n  mutate(across(everything(), ~gsub(\"\\n\", \"\", .))) %&gt;%\n  pivot_longer(everything(), names_to = \"Temperature\", values_to = \"Song\")  %&gt;%\n  group_by(Temperature, Song) %&gt;%\n  summarise(Count = n(), Percent = (Count / 200) * 100, .groups='drop')\n#\n\n# Create a bar chart\n\n# create labels for when there are enough respones\ndf_temperature_count &lt;- df_temperature_count %&gt;%\n  mutate(Label = ifelse(Count &gt; 5, as.character(Song), \"\"))\n\np &lt;- ggplot(df_temperature_count, aes(x = Temperature, y = Percent, fill = fct_reorder2(Song, Percent,Temperature))) +\n    geom_bar(stat= 'identity') +\n    geom_text(aes(label = Label), position = position_stack(vjust = 0.5), check_overlap = FALSE, angle=90) +\n    theme_economist() +\n    scale_fill_manual(values = c('#b8396b', '#ffd1d7', '#fff5cc', '#76bae0', '#b28f81', '#54483e'), guide = 'none') +\n    labs(x = \"\", \n         y = \"How often was a song identified (over 200 answers)\", \n         title = 'gpt-4o, tell me which Taylor Swift song is the \"best\"?',\n         #subtitle = 'When instructed to be less creative (meaning a lower temperature setting), the LLM responded more consistently.\\nAt the lowest creative setting (temperature of 0), the model answered \"All Too Well\"  for all 200 responses.',\n         caption = \"Chart colors inspired by the Lover album\") +\n    coord_flip() +\n    scale_x_discrete(labels = c(\"temperature_0\" = \"Lowest Creativity\", \n                                \"temperature_0.66\" = \"Low Creativity\", \n                                \"temperature_1.23\" = \"High Creativity\"),\n                     limits = c(\"temperature_1.23\", \"temperature_0.66\", \"temperature_0\")) +\n    scale_y_continuous(breaks = seq(0, 100, by = 10), labels = label_percent(scale = 1)) +\n    theme(axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0))) \n\nggsave(\"gpt-4o.png\", plot = p, width = 13, height = 8, units = \"in\", dpi = 300)\n\n\n\nI then looked at what were some of the most common responses at each temperature setting.\n\n\nhide / show code\n# Convert all columns to rows\ndf_gt &lt;- df %&gt;%\n    mutate(across(everything(), ~gsub(\"[[:punct:]]\", \"\", .))) %&gt;%\n    pivot_longer( cols = everything(), names_to = \"Column\", values_to = \"Value\") %&gt;%\n    count(Column, Value)  %&gt;%\n    arrange(desc(n))  %&gt;%\n    group_by(Column) %&gt;%\n    slice_max(n, n = 4) %&gt;%\n    mutate(percent_of_n = round(n / sum(n), 3)) %&gt;%\n    ungroup() \n\n\n\ndf_gt %&gt;%\n    mutate(Column = case_when(\n        Column == 'temperature_0' ~ 'Lowest Creativity',\n        Column == 'temperature_0.66' ~ 'Low Creativity',\n        Column == 'temperature_1.23' ~ 'High Creativity')\n    ) %&gt;%\n     gt(\n        rowname_col = \"Value\",\n        groupname_col = \"Column\"\n    )  %&gt;%\n    tab_header( title = \"gpt-4o - Best Taylor Swift Song by temperature setting\") %&gt;%\n    cols_width(\n        Column ~ pct(30), # Set the width of the \"Column\" column to 25%\n        Value ~ pct(30), # Set the width of the \"Value\" column to 35%\n        n ~ pct(20), # Set the width of the \"n\" column to 20%\n        percent_of_n ~ pct(20) # Set the width of the \"percent_of_n\" column to 20%\n    ) %&gt;%\n    fmt_percent(\n        columns = vars(percent_of_n),\n        decimals = 1 # Adjust the number of decimal places as needed\n    )  %&gt;%\n    cols_label(\n        Column = \"Temperature Setting\",\n        Value = \"Song\",\n        n = \"Times model picked song\",\n        percent_of_n = \"% of times picked\"\n    )  %&gt;%\n     tab_options(\n        row.striping.include_table_body = FALSE,\n        row_group.font.weight = \"bolder\",\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\ngpt-4o - Best Taylor Swift Song by temperature setting\n\n\n\nTimes model picked song\n% of times picked\n\n\n\n\nLowest Creativity\n\n\nAll Too Well\n200\n100.0%\n\n\nLow Creativity\n\n\nAll Too Well\n199\n99.5%\n\n\nBlank Space\n1\n0.5%\n\n\nHigh Creativity\n\n\nAll Too Well\n174\n87.4%\n\n\nBlank Space\n13\n6.5%\n\n\nLove Story\n8\n4.0%\n\n\nAll Too Well\n2\n1.0%\n\n\nShake It Off\n2\n1.0%\n\n\n\n\n\n\n\nAt the lowest setting, the model returned the exact same response for all 200 completions. As the temperature setting increased, the number of unique responses increased. I was surprised at the response of All Too well being the best song given I wasn’t too familiar with that song, but after some informal research (asking my wife and friends)realized it was critically acclaimed when it was re-released as a 10-minute version.\nUsing this latest model as a baseline, I wanted to see how different models might respond to the same question."
  },
  {
    "objectID": "posts/2024-07-19_llm-temperature-exploration/index.html#first-follow-up-test-asking-an-older-model-for-the-best-taylor-swift-song",
    "href": "posts/2024-07-19_llm-temperature-exploration/index.html#first-follow-up-test-asking-an-older-model-for-the-best-taylor-swift-song",
    "title": "Setting the tone: Exploring LLM Temperature Settings with Taylor Swift’s Songs",
    "section": "First follow up test: Asking an older model for the best Taylor Swift song",
    "text": "First follow up test: Asking an older model for the best Taylor Swift song\nI next asked an “older” model version the same question. For this, I used GPT-3.5 Turbo which released in March 2023 and trained on data up to September 2021.\n\n\nhide / show code\nmodel = 'gpt-3.5-turbo'\ntimes = 200\n\ntemp_values = [2, 1.98] \noutputs = {}\n\nfor x in temp_values:\n    outputs[f'temperature_{x}'] = run_batch_completions(x, prompt, sys_prompt, model, times)\n\n    # Pause for 60 seconds to avoid rate limit issues\n    time.sleep(120)\n\n\n# Convert the dictionary to a DataFrame\ndf = pd.DataFrame(outputs)\n\n# Write the DataFrame to a CSV file\ndf.to_csv('best_song_35turbo.csv', index=False)\n\n\n\n\nhide / show code\n#df &lt;- read.csv('posts/2024-06-19_llm-temperature-exploration/best_song_35turbo.csv')\ndf &lt;- read.csv('./best_song_35turbo.csv')\n\n# Aggregate data to count unique values for each temperature\ndf_temperature_count &lt;- df %&gt;%\n  mutate(across(everything(), ~gsub(\"[[:punct:]]\", \"\", .))) %&gt;%\n  mutate(across(everything(), ~gsub(\"\\n\", \"\", .))) %&gt;%\n  pivot_longer(everything(), names_to = \"Temperature\", values_to = \"Song\")  %&gt;%\n  group_by(Temperature, Song) %&gt;%\n  summarise(Count = n(), Percent = (Count / 200) * 100, .groups='drop')\n\n\n\n# Create a bar chart\ndf_temperature_count &lt;- df_temperature_count %&gt;%\n  mutate(Label = ifelse(Count &gt; 2, as.character(Song), \"\")) %&gt;%\n  mutate()\n\np &lt;- ggplot(df_temperature_count, aes(x = Temperature, y = Percent, fill = fct_reorder2(Song, Percent,Temperature))) +\n    geom_bar(stat= 'identity') +\n    geom_text(aes(label = Label), position = position_stack(vjust = 0.5), check_overlap = FALSE, angle=90) +\n    theme_economist() +\n    scale_fill_taylor(palette = \"lover\", guide='none') +\n    #scale_fill_manual(values = c('#b8396b', '#ffd1d7', '#fff5cc', '#76bae0', '#b28f81', '#54483e'), guide = 'none') +\n    labs(x = \"\", \n         y = \"How often was a song identified (over 200 answers)\", \n         title = 'gpt-3.5 Turbo, tell me which Taylor Swift song is the \"best\"?',\n         #subtitle = 'We see the same pattern with an older model, although there is more variation at higher creativity (including one Ariana Grande song, oddly). \\nInterestingly, this model was trained on data prior to All Too Well (10 Minute version), being released. That version was critically acclaimed, \\nso perhaps that is why \"All Too Well\" was not as highly rated by this version and Love Story was?',\n         caption = \"Chart colors inspired by the Lover album\") +\n    coord_flip() +\n    scale_x_discrete(labels = c(\"temperature_0\" = \"Lowest Creativity\", \n                                \"temperature_0.66\" = \"Low Creativity\", \n                                \"temperature_1.23\" = \"High Creativity\"),\n                     limits = c(\"temperature_1.23\", \"temperature_0.66\", \"temperature_0\")) +\n    scale_y_continuous(breaks = seq(0, 100, by = 10), labels = label_percent(scale = 1)) +\n    theme(axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0))) \n\nggsave(\"gpt-3.5-turbo.png\", plot = p, width = 13, height = 8, units = \"in\", dpi = 300)\n\n\n\nI immediately saw this model had more variation, including an odd response of No Tears Left to Cry  by Ariana Grande as the best Taylor Swift song (yes, the response included saying it was an Ariana Grande song). I again looked at the most common responses at each temperature setting.\n\n\nhide / show code\n# Convert all columns to rows\ndf_gt &lt;- df %&gt;%\n    mutate(across(everything(), ~gsub(\"[[:punct:]]\", \"\", .))) %&gt;%\n    pivot_longer( cols = everything(), names_to = \"Column\", values_to = \"Value\") %&gt;%\n    count(Column, Value)  %&gt;%\n    arrange(desc(n))  %&gt;%\n    group_by(Column) %&gt;%\n    #slice_max(n, n = 4) %&gt;%\n    mutate(percent_of_n = round(n / sum(n), 3)) %&gt;%\n    ungroup() \n\n\n\ndf_gt %&gt;%\n     mutate(Column = case_when(\n        Column == 'temperature_0' ~ 'Lowest Creativity',\n        Column == 'temperature_0.66' ~ 'Low Creativity',\n        Column == 'temperature_1.23' ~ 'Medium-High Creativity')\n    ) %&gt;%\n    gt(\n        rowname_col = \"Value\",\n        groupname_col = \"Column\"\n    )  %&gt;%\n    tab_header( title = \"gpt-3.5 Turbo - Best Taylor Swift Song by temperature setting\") %&gt;%\n    cols_width(\n        Column ~ pct(30), # Set the width of the \"Column\" column to 25%\n        Value ~ pct(30), # Set the width of the \"Value\" column to 35%\n        n ~ pct(20), # Set the width of the \"n\" column to 20%\n        percent_of_n ~ pct(20) # Set the width of the \"percent_of_n\" column to 20%\n    ) %&gt;%\n    fmt_percent(\n        columns = vars(percent_of_n),\n        decimals = 1 # Adjust the number of decimal places as needed\n    )  %&gt;%\n    cols_label(\n        Column = \"Temperature Setting\",\n        Value = \"Song\",\n        n = \"Count of Responses\",\n        percent_of_n = \"% of Responses\"\n    )  %&gt;%\n     tab_options(\n        row.striping.include_table_body = FALSE,\n        row_group.font.weight = \"bolder\",\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\ngpt-3.5 Turbo - Best Taylor Swift Song by temperature setting\n\n\n\nCount of Responses\n% of Responses\n\n\n\n\nLowest Creativity\n\n\nLove Story\n200\n100.0%\n\n\nLow Creativity\n\n\nLove Story\n190\n95.0%\n\n\nAll Too Well\n5\n2.5%\n\n\nBlank Space\n5\n2.5%\n\n\nMedium-High Creativity\n\n\nLove Story\n138\n69.0%\n\n\nAll Too Well\n20\n10.0%\n\n\nBlank Space\n18\n9.0%\n\n\nShake It Off\n13\n6.5%\n\n\nI Knew You Were Trouble\n1\n0.5%\n\n\nIn my opinion All Too Well is the best Taylor Swift song\n1\n0.5%\n\n\nNo Tears Left to Cry Ariana Grande\n1\n0.5%\n\n\nOpinions vary widely\n1\n0.5%\n\n\nSomebody To Love\n1\n0.5%\n\n\nSoon Youll Get Better\n1\n0.5%\n\n\nThats subjective but Love Story is a popular choice\n1\n0.5%\n\n\nThats subjective friend\n1\n0.5%\n\n\nThere are many options but some popular recommendations are Love Story Blank Space and Shake It Off\n1\n0.5%\n\n\nThis is subjective\n1\n0.5%\n\n\nWildest Dreams\n1\n0.5%\n\n\n\n\n\n\n\nThis older model did not rate All Too Well as highly as the newer model. This was interesting as the older model was trained on data up to September 2021, so it did not have access to the 10 minute version of All Too Well which was released in November 2021 and was critically acclaimed. Additionally, you can clearly see this older model had some odd outputs at higher temperatures."
  },
  {
    "objectID": "posts/2024-07-19_llm-temperature-exploration/index.html#second-follow-up-test-asking-claudes-latest-model-what-is-the-best-taylor-swift-song",
    "href": "posts/2024-07-19_llm-temperature-exploration/index.html#second-follow-up-test-asking-claudes-latest-model-what-is-the-best-taylor-swift-song",
    "title": "Setting the tone: Exploring LLM Temperature Settings with Taylor Swift’s Songs",
    "section": "Second follow up test: Asking Claude’s latest model what is the best Taylor Swift song",
    "text": "Second follow up test: Asking Claude’s latest model what is the best Taylor Swift song\nI haven’t used Anthropic’s models that much, but I wanted to see how it’s latest model would perform.\n\n\nhide / show code\nimport anthropic\nfrom secret_keys import CLAUDE_KEY\n\nclient = anthropic.Anthropic(\n    api_key=CLAUDE_KEY,\n)\n\n\ndef claude_completion(temp):\n    try:\n        message = client.messages.create(\n            model=\"claude-3-5-sonnet-20240620\",\n            max_tokens=1024,\n            system=sys_prompt,\n            temperature=temp,\n            messages=[\n                {\"role\": \"user\", \"content\": prompt},\n            ]\n        )\n        return message.content[0].text\n    except Exception as e:\n        return f\"error - {str(e)}\"\n\ndef claude_batch_completions(temp, num_responses):\n    responses = []\n    for _ in range(num_responses):\n        response = claude_completion(temp)\n        responses.append(response)\n    return pd.Series(responses)\n\ntimes = 200\n\ntemp_values = [0, 0.33, 0.66, 1]\noutputs = {}\n\nfor x in temp_values:\n    print('starting' + str(x))\n    outputs[f'temperature_{x}'] = claude_batch_completions(x, times)\n    # Pause for 60 seconds to avoid rate limit issues\n    print('sleeping')\n    time.sleep(60)\n\n\n# Convert the dictionary to a DataFrame\ndf = pd.DataFrame(outputs)\n\n# Write the DataFrame to a CSV file\ndf.to_csv('best_song_claude.csv', index=False)\n\n\nImmediately what stood out was this model was able to produce valid results at all temperatures. I didn’t mention it, but the gpt models all hallucinated at the highest temperature setting that I had to throw out the results. These answer weren’t just wrong (like listing an Arianna Grande song), but they were nonsensical. Some include json formatted data, foreign languages, and other oddities. Claude returned valid responses at all temperatures.\n\n\nhide / show code\n#df &lt;- read.csv('posts/2024-06-19_llm-temperature-exploration/best_song_claude.csv')\ndf &lt;- read.csv('./best_song_claude.csv')\n\n\n# Aggregate data to count unique values for each temperature\ndf_temperature_count &lt;- df %&gt;%\n  mutate(across(everything(), ~gsub(\"[[:punct:]]\", \"\", .))) %&gt;%\n  mutate(across(everything(), ~gsub(\"\\n\", \"\", .))) %&gt;%\n  pivot_longer(everything(), names_to = \"Temperature\", values_to = \"Song\")  %&gt;%\n  group_by(Temperature, Song) %&gt;%\n  summarise(Count = n(), Percent = (Count / 200) * 100, .groups='drop')\n#\n\n# Create a bar chart\ndf_temperature_count &lt;- df_temperature_count %&gt;%\n  mutate(Label = ifelse(Count &gt; 2, as.character(Song), \"\"))\n  \np &lt;- ggplot(df_temperature_count, aes(x = Temperature, y = Percent, fill = Song)) +\n    geom_bar(stat = \"identity\") +\n    geom_text(aes(label = Label), position = position_stack(vjust = 0.5), check_overlap = FALSE, angle=90) +\n    theme_economist() +\n    #scale_fill_taylor(palette = \"taylor1989\", guide='none') +\n    scale_fill_manual(values = c('#b8396b', '#ffd1d7', '#fff5cc', '#76bae0', '#b28f81', '#54483e'), guide = 'none') +\n    labs(x = \"\", \n         y = \"How often was a song identified (over 200 answers)\", \n         title = 'Claude Sonnet, which Taylor Swift song is the \"best\"?',\n         #subtitle = 'We see the same pattern with this latest model from Anthropic, although it identified \"Shake it Off\" as the \"best\". \\nThis model was trained on data up to 2024, so it is a timely reminder that different models can have access to similar data but\\narrive at different conclusions based on how they are built and the impact of the language we use with LLMs (in this case, what does the best mean?).',\n         caption = \"Chart colors inspired by the Lover album\") +\n    coord_flip() + \n    scale_x_discrete(labels = c(\"temperature_0\" = \"Lowest Creativity\", \n                                \"temperature_0.33\" = \"Low-MediumCreativity\", \n                                \"temperature_0.66\" = \"Medium-High Creativity\",\n                                \"temperature_1\" = \"Highest Creativity\"),\n                     limits = c(\"temperature_1\", \"temperature_0.66\", 'temperature_0.33',\"temperature_0\")) +\n    theme(axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0))) \n\nggsave(\"claude.png\", plot = p, width = 13, height = 8, units = \"in\", dpi = 300)\n\n\n\n\n\nhide / show code\n# Convert all columns to rows\ndf_gt &lt;- df %&gt;%\n    mutate(across(everything(), ~gsub(\"[[:punct:]]\", \"\", .))) %&gt;%\n    pivot_longer( cols = everything(), names_to = \"Column\", values_to = \"Value\") %&gt;%\n    count(Column, Value)  %&gt;%\n    arrange(desc(n))  %&gt;%\n    group_by(Column) %&gt;%\n    slice_max(n, n = 4) %&gt;%\n    mutate(percent_of_n = round(n / sum(n), 3)) %&gt;%\n    ungroup() \n\n\n\ndf_gt %&gt;%\n    mutate(Column = case_when(\n        Column == \"temperature_0\" ~ \"Lowest_Creativity\",\n        Column == \"temperature_0.33\" ~ \"Low_MediumCreativity\",\n        Column == \"temperature_0.66\" ~ \"Medium_High_Creativity\",\n        Column == \"temperature_1\" ~ \"Highest_Creativity\")\n        ) %&gt;%\n     gt(\n        rowname_col = \"Value\",\n        groupname_col = \"Column\"\n    )  %&gt;%\n    tab_header( title = \"Claude Sonnet - Best Taylor Swift Song by temperature setting\") %&gt;%\n    cols_width(\n        Column ~ pct(30), # Set the width of the \"Column\" column to 25%\n        Value ~ pct(30), # Set the width of the \"Value\" column to 35%\n        n ~ pct(20), # Set the width of the \"n\" column to 20%\n        percent_of_n ~ pct(20) # Set the width of the \"percent_of_n\" column to 20%\n    ) %&gt;%\n    fmt_percent(\n        columns = vars(percent_of_n),\n        decimals = 1 # Adjust the number of decimal places as needed\n    )  %&gt;%\n    cols_label(\n        Column = \"Temperature Setting\",\n        Value = \"Song\",\n        n = \"Count of Responses\",\n        percent_of_n = \"% of Responses\"\n    )  %&gt;%\n     tab_options(\n        row.striping.include_table_body = FALSE,\n        row_group.font.weight = \"bolder\",\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\nClaude Sonnet - Best Taylor Swift Song by temperature setting\n\n\n\nCount of Responses\n% of Responses\n\n\n\n\nLowest_Creativity\n\n\nShake It Off\n200\n100.0%\n\n\nLow_MediumCreativity\n\n\nShake It Off\n184\n92.0%\n\n\nAll Too Well\n12\n6.0%\n\n\nBlank Space\n4\n2.0%\n\n\nMedium_High_Creativity\n\n\nShake It Off\n130\n65.0%\n\n\nAll Too Well\n49\n24.5%\n\n\nBlank Space\n17\n8.5%\n\n\nCruel Summer\n4\n2.0%\n\n\nHighest_Creativity\n\n\nShake It Off\n96\n48.5%\n\n\nAll Too Well\n59\n29.8%\n\n\nBlank Space\n33\n16.7%\n\n\nCruel Summer\n10\n5.1%"
  },
  {
    "objectID": "posts/2024-07-19_llm-temperature-exploration/index.html#takeways",
    "href": "posts/2024-07-19_llm-temperature-exploration/index.html#takeways",
    "title": "Setting the tone: Exploring LLM Temperature Settings with Taylor Swift’s Songs",
    "section": "Takeways",
    "text": "Takeways\nFar from a scientific approach but this did help me better understand how temperature impacts short LLM responses. I would like to try this again but perhaps looking at how the responses vary with longer answers."
  },
  {
    "objectID": "posts/2024-07-19_llm-temperature-exploration/index.html#conclusion",
    "href": "posts/2024-07-19_llm-temperature-exploration/index.html#conclusion",
    "title": "Setting the tone: Exploring LLM Temperature Settings with Taylor Swift’s Songs",
    "section": "Conclusion",
    "text": "Conclusion\nThis was far from a scientific approach, but it did help me better understand how temperature impacts short LLM responses. For a future post, I want to try this again but look at how the responses vary with longer answers."
  }
]